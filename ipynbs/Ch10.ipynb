{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geBCbDunfwUW"
   },
   "source": [
    "# 第10章: 機械翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLb3iWZ3fzSs"
   },
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/altair626/.local/lib/python3.6/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'ja_ginza' (3.1.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/altair626/.local/lib/python3.6/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_ja = spacy.load('ja_ginza')\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 私 私 PRON 代名詞 iobj 5\n",
      "1 は は ADP 助詞-係助詞 case 0\n",
      "2 昨日 昨日 ADV 名詞-普通名詞-副詞可能 advmod 5\n",
      "3 寿司 寿司 NOUN 名詞-普通名詞-一般 obj 5\n",
      "4 を を ADP 助詞-格助詞 case 3\n",
      "5 食べ 食べる VERB 動詞-一般 ROOT 5\n",
      "6 まし ます AUX 助動詞 aux 5\n",
      "7 た た AUX 助動詞 aux 5\n",
      "8 。 。 PUNCT 補助記号-句点 punct 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_ja('私は昨日寿司を食べました。')\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, \n",
    "              token.tag_, token.dep_, token.head.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar xzvf kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('./kftt-data-1.0/data/orig/kyoto-'+fname+'.ja', \"r\") as intxt, open(\"./\"+fname+\".ja\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            doc = nlp_ja(txt)\n",
    "            mrphs = [token.orth_ for sent in doc.sents for token in sent]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('./kftt-data-1.0/data/orig/kyoto-'+fname+'.en', \"r\") as intxt, open(\"./\"+fname+\".en\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            pt = nlp_en.make_doc(txt)\n",
    "            mrphs = [doc.text for doc in pt]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雪舟 （ せっ しゅう 、 1420 年 （ 応永 27 年 ） - 1506 年 （ 永正 3 年 ） ） は 号 で 、 15 世紀 後半 室町 時代 に 活躍 し た 水墨 画家 ・ 禅僧 で 、 画聖 と も 称え られる 。\r\n",
      "日本 の 水墨画 を 一変 さ せ た 。\r\n",
      "諱 は 「 等楊 （ とう よう ） 」 、 もしくは 「 拙 宗 （ せっ しゅう ） 」 と 号し た 。\r\n",
      "備中 国 に 生まれ 、 京都 ・ 相国 寺 に 入っ て から 周防 国 に 移る 。\r\n",
      "その 後 遣明 使 に 随行 し て 中国 （ 明 ） に 渡っ て 中国 の 水墨画 を 学ん だ 。\r\n",
      "作品 は 数多く 、 中国 風 の 山水画 だけ で なく 人物画 や 花鳥画 も よく し た 。\r\n",
      "大胆 な 構図 と 力強い 筆線 は 非常 に 個性的 な 画風 を 作り出し て いる 。\r\n",
      "現存 する 作品 の うち 6 点 が 国宝 に 指定 さ れ て おり 、 日本 の 画家 の なか で も 別格 の 評価 を 受け て いる と いえる 。\r\n",
      "この ため 、 花鳥 図 屏風 など に 「 伝 雪舟 筆 」 さ れる 作品 は 大変 多い 。\r\n",
      "真筆 で ある か 専門家 の 間 で も 意見 の 分かれる もの も 多々 ある 。\r\n"
     ]
    }
   ],
   "source": [
    "!head train.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known as Sesshu ( 1420 - 1506 ) , he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century , and was called a master painter .\r\n",
      "He revolutionized the Japanese ink painting .\r\n",
      "He was given the posthumous name \" Toyo \" or \" Sesshu ( 拙宗 ) . \"\r\n",
      "Born in Bicchu Province , he moved to Suo Province after entering SShokoku - ji Temple in Kyoto .\r\n",
      "Later he accompanied a mission to Ming Dynasty China and learned Chinese ink painting .\r\n",
      "His works were many , including not only Chinese - style landscape paintings , but also portraits and pictures of flowers and birds .\r\n",
      "His bold compositions and strong brush strokes constituted an extremely distinctive style .\r\n",
      "6 of his extant works are designated national treasures . Indeed , he is considered to be extraordinary among Japanese painters .\r\n",
      "For this reason , there are a great many artworks that are attributed to him , such as folding screens with pictures of flowers and that birds are painted on them .\r\n",
      "There are many works that even experts can not agree if they are really his work or not .\r\n"
     ]
    }
   ],
   "source": [
    "!head train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VFkJlBEf2Uk"
   },
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "    --trainpref train \\\n",
    "    --validpref dev \\\n",
    "    --testpref test \\\n",
    "    --destdir ch10  \\\n",
    "    --workers 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train ch10 \\\n",
    "    --fp16 \\\n",
    "    --save-dir ch10-save \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 2 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 2000 --num-workers 10 > 91.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Hzf5EQEf48U"
   },
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-interactive --path ch10-save/checkpoint10.pt ch10 < test.ja | grep '^H' | cut -f3 > 92.hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JjYxtX-f7jM"
   },
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-score --sys 92.hypothesis --ref test.en\n",
    "#BLEU4 = 22.07, 51.7/26.9/16.3/10.5 (BP=1.000, ratio=1.087, syslen=30033, reflen=27628)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/moses-smt/mosesdecoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ./mosesdecoder/scripts/generic/multi-bleu.perl test.en < 92.hypothesis\n",
    "#Namespace(ignore_case=False, order=4, ref='test.en', sacrebleu=False, sentence_bleu=False, sys='92.hypothesis') BLEU4 = 22.07, 51.7/26.9/16.3/10.5 (BP=1.000, ratio=1.087, syslen=30033, reflen=27628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCvxtti8f-CL"
   },
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for N in `seq 5 15` ; do\n",
    "    fairseq-interactive --path ch10-save/checkpoint10.pt --beam $N ch10 < test.ja | grep '^H' | cut -f3 > 94.out\n",
    "    echo $N\n",
    "    fairseq-score --sys 94.out --ref test.en >> scores94\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./drive/My Drive/Colab Notebooks/data/Ch10/scores94\", \"r\") as intxt:\n",
    "  scores = intxt.readlines()\n",
    "\n",
    "scores = [i[8:13] for i in scores[1::2]]\n",
    "xs = [i+1 for i in range(len(scores))]\n",
    "\n",
    "plt.plot(xs, scores)\n",
    "plt.show()\n",
    "# download.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ORRGeC6gAYb"
   },
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ch10_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DIR=ch10_bpe\n",
    "git clone https://github.com/rsennrich/subword-nmt.git\n",
    "BPEROOT=subword-nmt/subword_nmt\n",
    "BPE_TOKENS=16000\n",
    "BPE_CODE=$DIR/learn_bpe.en\n",
    "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < train.en > $BPE_CODE\n",
    "\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < train.en > $DIR/train.en\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < test.en > $DIR/test.en\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < dev.en > $DIR/valid.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DIR=ch10_bpe\n",
    "BPEROOT=subword-nmt/subword_nmt\n",
    "BPE_TOKENS=16000\n",
    "BPE_CODE=$DIR/learn_bpe.ja\n",
    "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < train.ja > $BPE_CODE\n",
    "\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < train.ja > $DIR/train.ja\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < test.ja > $DIR/test.ja\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < dev.ja > $DIR/valid.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "    --trainpref ch10_bpe/train \\\n",
    "    --validpref ch10_bpe/valid \\\n",
    "    --testpref ch10_bpe/test \\\n",
    "    --destdir ch10_bpe_pp  \\\n",
    "    --workers 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 fairseq-train ch10_bpe_pp \\\n",
    "    --fp16 \\\n",
    "    --save-dir ch10_bpe-save \\\n",
    "    --tensorboard-logdir ch10_bpe-log \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 2 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 --num-workers 20 > 95.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-interactive --path ch10_bpe-save/checkpoint10.pt ch10_bpe_pp < test.ja | grep '^H' | cut -f3 > 95.hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-score --sys 95.hypothesis --ref test.en\n",
    "# Namespace(ignore_case=False, order=4, ref='test.en', sacrebleu=False, sentence_bleu=False, sys='95.hypothesis')\n",
    "#BLEU4 = 15.56, 43.3/19.7/10.7/6.4 (BP=1.000, ratio=1.107, syslen=30596, reflen=27628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7tQOmZIgELz"
   },
   "source": [
    "## 96. 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ch10_bpe-log\n",
    "# 1.PNG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbYVowYqgFzL"
   },
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RiMkQvPgIIz"
   },
   "source": [
    "## 98. ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBHudOURgLgr"
   },
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npZakVeBffuF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
