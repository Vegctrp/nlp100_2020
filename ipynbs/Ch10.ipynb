{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geBCbDunfwUW"
   },
   "source": [
    "# 第10章: 機械翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLb3iWZ3fzSs"
   },
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'ja_ginza' (3.1.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp_ja = spacy.load('ja_ginza')\n",
    "nlp_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 私 私 PRON 代名詞 iobj 5\n",
      "1 は は ADP 助詞-係助詞 case 0\n",
      "2 昨日 昨日 ADV 名詞-普通名詞-副詞可能 advmod 5\n",
      "3 寿司 寿司 NOUN 名詞-普通名詞-一般 obj 5\n",
      "4 を を ADP 助詞-格助詞 case 3\n",
      "5 食べ 食べる VERB 動詞-一般 ROOT 5\n",
      "6 まし ます AUX 助動詞 aux 5\n",
      "7 た た AUX 助動詞 aux 5\n",
      "8 。 。 PUNCT 補助記号-句点 punct 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_ja('私は昨日寿司を食べました。')\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, \n",
    "              token.tag_, token.dep_, token.head.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kftt-data-1.0/\n",
      "kftt-data-1.0/data/\n",
      "kftt-data-1.0/data/orig/\n",
      "kftt-data-1.0/data/orig/kyoto-tune.en\n",
      "kftt-data-1.0/data/orig/kyoto-dev.ja\n",
      "kftt-data-1.0/data/orig/kyoto-dev.en\n",
      "kftt-data-1.0/data/orig/kyoto-train.en\n",
      "kftt-data-1.0/data/orig/kyoto-tune.ja\n",
      "kftt-data-1.0/data/orig/kyoto-train.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.ja\n",
      "kftt-data-1.0/data/orig/kyoto-test.en\n",
      "kftt-data-1.0/data/tok/\n",
      "kftt-data-1.0/data/tok/kyoto-tune.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.en\n",
      "kftt-data-1.0/data/tok/kyoto-dev.en\n",
      "kftt-data-1.0/data/tok/kyoto-train.en\n",
      "kftt-data-1.0/data/tok/kyoto-tune.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.cln.ja\n",
      "kftt-data-1.0/data/tok/kyoto-train.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.ja\n",
      "kftt-data-1.0/data/tok/kyoto-test.en\n",
      "kftt-data-1.0/README.txt\n"
     ]
    }
   ],
   "source": [
    "!tar xzvf kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440288/440288 [1:13:14<00:00, 100.20it/s]\n",
      "100%|██████████| 1160/1160 [00:10<00:00, 106.19it/s]\n",
      "100%|██████████| 1166/1166 [00:10<00:00, 110.14it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('./kftt-data-1.0/data/orig/kyoto-'+fname+'.ja', \"r\") as intxt, open(\"./\"+fname+\".ja\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            doc = nlp_ja(txt)\n",
    "            mrphs = [token.orth_ for sent in doc.sents for token in sent]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440288/440288 [01:38<00:00, 4476.23it/s]\n",
      "100%|██████████| 1160/1160 [00:00<00:00, 4554.35it/s]\n",
      "100%|██████████| 1166/1166 [00:00<00:00, 5201.26it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('./kftt-data-1.0/data/orig/kyoto-'+fname+'.en', \"r\") as intxt, open(\"./\"+fname+\".en\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            pt = nlp_en.make_doc(txt)\n",
    "            mrphs = [doc.text for doc in pt]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雪舟 （ せっ しゅう 、 1420 年 （ 応永 27 年 ） - 1506 年 （ 永正 3 年 ） ） は 号 で 、 15 世紀 後半 室町 時代 に 活躍 し た 水墨 画家 ・ 禅僧 で 、 画聖 と も 称え られる 。\n",
      "日本 の 水墨画 を 一変 さ せ た 。\n",
      "諱 は 「 等楊 （ とう よう ） 」 、 もしくは 「 拙 宗 （ せっ しゅう ） 」 と 号し た 。\n",
      "備中 国 に 生まれ 、 京都 ・ 相国 寺 に 入っ て から 周防 国 に 移る 。\n",
      "その 後 遣明 使 に 随行 し て 中国 （ 明 ） に 渡っ て 中国 の 水墨画 を 学ん だ 。\n",
      "作品 は 数多く 、 中国 風 の 山水画 だけ で なく 人物画 や 花鳥画 も よく し た 。\n",
      "大胆 な 構図 と 力強い 筆線 は 非常 に 個性的 な 画風 を 作り出し て いる 。\n",
      "現存 する 作品 の うち 6 点 が 国宝 に 指定 さ れ て おり 、 日本 の 画家 の なか で も 別格 の 評価 を 受け て いる と いえる 。\n",
      "この ため 、 花鳥 図 屏風 など に 「 伝 雪舟 筆 」 さ れる 作品 は 大変 多い 。\n",
      "真筆 で ある か 専門家 の 間 で も 意見 の 分かれる もの も 多々 ある 。\n"
     ]
    }
   ],
   "source": [
    "!head train.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known as Sesshu ( 1420 - 1506 ) , he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century , and was called a master painter .\n",
      "He revolutionized the Japanese ink painting .\n",
      "He was given the posthumous name \" Toyo \" or \" Sesshu ( 拙宗 ) . \"\n",
      "Born in Bicchu Province , he moved to Suo Province after entering SShokoku - ji Temple in Kyoto .\n",
      "Later he accompanied a mission to Ming Dynasty China and learned Chinese ink painting .\n",
      "His works were many , including not only Chinese - style landscape paintings , but also portraits and pictures of flowers and birds .\n",
      "His bold compositions and strong brush strokes constituted an extremely distinctive style .\n",
      "6 of his extant works are designated national treasures . Indeed , he is considered to be extraordinary among Japanese painters .\n",
      "For this reason , there are a great many artworks that are attributed to him , such as folding screens with pictures of flowers and that birds are painted on them .\n",
      "There are many works that even experts can not agree if they are really his work or not .\n"
     ]
    }
   ],
   "source": [
    "!head train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VFkJlBEf2Uk"
   },
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='ch10', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='ja', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref='test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, trainpref='train', user_dir=None, validpref='dev', workers=5)\n",
      "| [ja] Dictionary: 60247 types\n",
      "| [ja] train.ja: 440288 sents, 11273745 tokens, 1.43% replaced by <unk>\n",
      "| [ja] Dictionary: 60247 types\n",
      "| [ja] dev.ja: 1166 sents, 25519 tokens, 1.55% replaced by <unk>\n",
      "| [ja] Dictionary: 60247 types\n",
      "| [ja] test.ja: 1160 sents, 26918 tokens, 1.72% replaced by <unk>\n",
      "| [en] Dictionary: 55511 types\n",
      "| [en] train.en: 440288 sents, 12320121 tokens, 1.58% replaced by <unk>\n",
      "| [en] Dictionary: 55511 types\n",
      "| [en] dev.en: 1166 sents, 26093 tokens, 2.85% replaced by <unk>\n",
      "| [en] Dictionary: 55511 types\n",
      "| [en] test.en: 1160 sents, 28788 tokens, 2.03% replaced by <unk>\n",
      "| Wrote preprocessed data to ch10\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "    --trainpref train \\\n",
    "    --validpref dev \\\n",
    "    --testpref test \\\n",
    "    --destdir ch10  \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/miyazaki.k/kemvenv/bin/fairseq-train\", line 11, in <module>\n",
      "    load_entry_point('fairseq==0.9.0', 'console_scripts', 'fairseq-train')()\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 329, in cli_main\n",
      "    nprocs=args.distributed_world_size,\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n",
      "    while not spawn_context.join():\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n",
      "    raise Exception(msg)\n",
      "Exception: \n",
      "\n",
      "-- Process 3 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 296, in distributed_main\n",
      "    main(args, init_distributed=True)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 61, in main\n",
      "    trainer = Trainer(args, task, model, criterion)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq/trainer.py\", line 46, in __init__\n",
      "    self._model = self._model.cuda()\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 304, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 223, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 304, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 3; 10.76 GiB total capacity; 60.84 MiB already allocated; 4.56 MiB free; 64.00 MiB reserved in total by PyTorch)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=5,6,7,8 fairseq-train ch10 \\\n",
    "    --fp16 \\\n",
    "    --save-dir ch10-save \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 2000 > 91.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Hzf5EQEf48U"
   },
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-interactive --path ch10-save/checkpoint10.pt ch10 < test.ja | grep '^H' | cut -f3 > 92.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!fairseq-generate ch10 --path ch10-save/checkpoint10.pt \\\n",
    "    --batch-size 128 --beam 4 > honyaku-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat honyaku-out | grep ^T | cut -f2- > target.txt\n",
    "!cat honyaku-out | grep ^H | cut -f3- > hypothesis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n",
      "1864\n",
      "1867\n",
      "Land - based Digital Television Broadcasting Facilities\n",
      "Japanese National Railways Steam Locomotive Class <<unk>>\n",
      "<<unk>> in 1925 by Hitachi , Ltd.\n",
      "<<unk>> in 1948 by Hitachi , Ltd.\n",
      "Transferred from the Miyazaki Engine Depot .\n",
      "Volume 4 consists of 32 chapters .\n",
      "Volume 1 consists of 107 chapters .\n"
     ]
    }
   ],
   "source": [
    "!head target.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1866\n",
      "1864\n",
      "1867\n",
      "<unk> TV broadcasting facilities\n",
      "JNR / JR ( JNR ) <unk> steam locomotive\n",
      "It was manufactured by Hitachi Hitachi in 1925 .\n",
      "It was manufactured by Hitachi Hitachi in 1948 .\n",
      "He was transferred from Miyazaki Depot .\n",
      "It consists of four stories and 32 stories .\n",
      "One volume , 107 stories .\n"
     ]
    }
   ],
   "source": [
    "!head hypothesis.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mosesdecoder'...\n",
      "remote: Enumerating objects: 46, done.\u001b[K\n",
      "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 147560 (delta 20), reused 18 (delta 7), pack-reused 147514\u001b[K\n",
      "Receiving objects: 100% (147560/147560), 129.76 MiB | 16.32 MiB/s, done.\n",
      "Resolving deltas: 100% (114006/114006), done.\n",
      "Checking out files: 100% (3467/3467), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/moses-smt/mosesdecoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 21.98, 53.3/27.2/16.0/10.1 (BP=1.000, ratio=1.021, hyp_len=28218, ref_len=27628)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    }
   ],
   "source": [
    "!perl ./mosesdecoder/scripts/generic/multi-bleu.perl target.txt < hypothesis.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JjYxtX-f7jM"
   },
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(ignore_case=False, order=4, ref='test.en', sacrebleu=False, sentence_bleu=False, sys='92.out')\n",
      "BLEU4 = 21.99, 53.1/27.2/16.0/10.1 (BP=1.000, ratio=1.026, syslen=28360, reflen=27628)\n"
     ]
    }
   ],
   "source": [
    "!fairseq-score --sys 92.out --ref test.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCvxtti8f-CL"
   },
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ORRGeC6gAYb"
   },
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ch10_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'subword-nmt'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DIR=ch10_bpe\n",
    "TRAIN=$DIR/train.ja-en\n",
    "cat train.ja > $TRAIN\n",
    "cat train.en >> $TRAIN\n",
    "\n",
    "git clone https://github.com/rsennrich/subword-nmt.git\n",
    "BPEROOT=subword-nmt/subword_nmt\n",
    "BPE_TOKENS=40000\n",
    "BPE_CODE=$DIR/learn_bpe.jaen\n",
    "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $TRAIN > $BPE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DIR=ch10_bpe\n",
    "BPEROOT=subword-nmt/subword_nmt\n",
    "BPE_TOKENS=40000\n",
    "BPE_CODE=$DIR/learn_bpe.jaen\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < train.ja > $DIR/train.ja\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < train.en > $DIR/train.en\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < test.ja > $DIR/test.ja\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < test.en > $DIR/test.en\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < dev.ja > $DIR/valid.ja\n",
    "python $BPEROOT/apply_bpe.py -c $BPE_CODE < dev.en > $DIR/valid.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='ch10_bpe_pp', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='ja', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref='ch10_bpe/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, trainpref='ch10_bpe/train', user_dir=None, validpref='ch10_bpe/valid', workers=5)\n",
      "| [ja] Dictionary: 24583 types\n",
      "| [ja] ch10_bpe/train.ja: 440288 sents, 12200780 tokens, 0.077% replaced by <unk>\n",
      "| [ja] Dictionary: 24583 types\n",
      "| [ja] ch10_bpe/valid.ja: 1166 sents, 27704 tokens, 0.0794% replaced by <unk>\n",
      "| [ja] Dictionary: 24583 types\n",
      "| [ja] ch10_bpe/test.ja: 1160 sents, 29139 tokens, 0.0995% replaced by <unk>\n",
      "| [en] Dictionary: 26239 types\n",
      "| [en] ch10_bpe/train.en: 440288 sents, 13385684 tokens, 0.0761% replaced by <unk>\n",
      "| [en] Dictionary: 26239 types\n",
      "| [en] ch10_bpe/valid.en: 1166 sents, 28934 tokens, 0.0933% replaced by <unk>\n",
      "| [en] Dictionary: 26239 types\n",
      "| [en] ch10_bpe/test.en: 1160 sents, 31331 tokens, 0.118% replaced by <unk>\n",
      "| Wrote preprocessed data to ch10_bpe_pp\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "    --trainpref ch10_bpe/train \\\n",
    "    --validpref ch10_bpe/valid \\\n",
    "    --testpref ch10_bpe/test \\\n",
    "    --destdir ch10_bpe_pp  \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/miyazaki.k/kemvenv/bin/fairseq-train\", line 11, in <module>\n",
      "    load_entry_point('fairseq==0.9.0', 'console_scripts', 'fairseq-train')()\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 329, in cli_main\n",
      "    nprocs=args.distributed_world_size,\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n",
      "    while not spawn_context.join():\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 118, in join\n",
      "    raise Exception(msg)\n",
      "Exception: \n",
      "\n",
      "-- Process 3 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 19, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 296, in distributed_main\n",
      "    main(args, init_distributed=True)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq_cli/train.py\", line 61, in main\n",
      "    trainer = Trainer(args, task, model, criterion)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/fairseq/trainer.py\", line 46, in __init__\n",
      "    self._model = self._model.cuda()\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 304, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 201, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 223, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 304, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 3; 10.76 GiB total capacity; 56.08 MiB already allocated; 8.56 MiB free; 60.00 MiB reserved in total by PyTorch)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=5,6,7,8 fairseq-train ch10_bpe_pp \\\n",
    "    --fp16 \\\n",
    "    --save-dir ch10_bpe-save \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 2000 > 95.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7tQOmZIgELz"
   },
   "source": [
    "## 96. 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbYVowYqgFzL"
   },
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RiMkQvPgIIz"
   },
   "source": [
    "## 98. ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBHudOURgLgr"
   },
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npZakVeBffuF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
