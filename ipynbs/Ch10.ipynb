{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geBCbDunfwUW"
   },
   "source": [
    "# 第10章: 機械翻訳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLb3iWZ3fzSs"
   },
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'ja_ginza' (3.1.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6aed3e3aa216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnlp_ja\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ja_ginza'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp_ja = spacy.load('ja_ginza')\n",
    "nlp_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 私 私 PRON 代名詞 iobj 5\n",
      "1 は は ADP 助詞-係助詞 case 0\n",
      "2 昨日 昨日 ADV 名詞-普通名詞-副詞可能 advmod 5\n",
      "3 寿司 寿司 NOUN 名詞-普通名詞-一般 obj 5\n",
      "4 を を ADP 助詞-格助詞 case 3\n",
      "5 食べ 食べる VERB 動詞-一般 ROOT 5\n",
      "6 まし ます AUX 助動詞 aux 5\n",
      "7 た た AUX 助動詞 aux 5\n",
      "8 。 。 PUNCT 補助記号-句点 punct 5\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_ja('私は昨日寿司を食べました。')\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, \n",
    "              token.tag_, token.dep_, token.head.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440288/440288 [1:47:09<00:00, 68.48it/s]  \n",
      "100%|██████████| 1160/1160 [00:16<00:00, 69.34it/s]\n",
      "100%|██████████| 1166/1166 [00:15<00:00, 74.80it/s] \n"
     ]
    }
   ],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('../data/kftt-data-1.0/data/orig/kyoto-'+fname+'.ja', \"r\") as intxt, open(\"./\"+fname+\".ja\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            doc = nlp_ja(txt)\n",
    "            mrphs = [token.orth_ for sent in doc.sents for token in sent]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440288/440288 [01:50<00:00, 3971.87it/s]\n",
      "100%|██████████| 1160/1160 [00:00<00:00, 3608.52it/s]\n",
      "100%|██████████| 1166/1166 [00:00<00:00, 4170.07it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\"train\",\"test\", \"dev\"]\n",
    "\n",
    "for fname in files:\n",
    "    with open('../data/kftt-data-1.0/data/orig/kyoto-'+fname+'.en', \"r\") as intxt, open(\"./\"+fname+\".en\", \"w\") as out:\n",
    "        txts = intxt.readlines()\n",
    "        for txt in tqdm(txts):\n",
    "            txt = txt.strip(\"\\n\")\n",
    "            pt = nlp_en.make_doc(txt)\n",
    "            mrphs = [doc.text for doc in pt]\n",
    "            print(\" \".join(mrphs), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雪舟 （ せっ しゅう 、 1420 年 （ 応永 27 年 ） - 1506 年 （ 永正 3 年 ） ） は 号 で 、 15 世紀 後半 室町 時代 に 活躍 し た 水墨 画家 ・ 禅僧 で 、 画聖 と も 称え られる 。\r\n",
      "日本 の 水墨画 を 一変 さ せ た 。\r\n",
      "諱 は 「 等楊 （ とう よう ） 」 、 もしくは 「 拙 宗 （ せっ しゅう ） 」 と 号し た 。\r\n",
      "備中 国 に 生まれ 、 京都 ・ 相国 寺 に 入っ て から 周防 国 に 移る 。\r\n",
      "その 後 遣明 使 に 随行 し て 中国 （ 明 ） に 渡っ て 中国 の 水墨画 を 学ん だ 。\r\n",
      "作品 は 数多く 、 中国 風 の 山水画 だけ で なく 人物画 や 花鳥画 も よく し た 。\r\n",
      "大胆 な 構図 と 力強い 筆線 は 非常 に 個性的 な 画風 を 作り出し て いる 。\r\n",
      "現存 する 作品 の うち 6 点 が 国宝 に 指定 さ れ て おり 、 日本 の 画家 の なか で も 別格 の 評価 を 受け て いる と いえる 。\r\n",
      "この ため 、 花鳥 図 屏風 など に 「 伝 雪舟 筆 」 さ れる 作品 は 大変 多い 。\r\n",
      "真筆 で ある か 専門家 の 間 で も 意見 の 分かれる もの も 多々 ある 。\r\n"
     ]
    }
   ],
   "source": [
    "!head train.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known as Sesshu ( 1420 - 1506 ) , he was an ink painter and Zen monk active in the Muromachi period in the latter half of the 15th century , and was called a master painter .\r\n",
      "He revolutionized the Japanese ink painting .\r\n",
      "He was given the posthumous name \" Toyo \" or \" Sesshu ( 拙宗 ) . \"\r\n",
      "Born in Bicchu Province , he moved to Suo Province after entering SShokoku - ji Temple in Kyoto .\r\n",
      "Later he accompanied a mission to Ming Dynasty China and learned Chinese ink painting .\r\n",
      "His works were many , including not only Chinese - style landscape paintings , but also portraits and pictures of flowers and birds .\r\n",
      "His bold compositions and strong brush strokes constituted an extremely distinctive style .\r\n",
      "6 of his extant works are designated national treasures . Indeed , he is considered to be extraordinary among Japanese painters .\r\n",
      "For this reason , there are a great many artworks that are attributed to him , such as folding screens with pictures of flowers and that birds are painted on them .\r\n",
      "There are many works that even experts can not agree if they are really his work or not .\r\n"
     ]
    }
   ],
   "source": [
    "!head train.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VFkJlBEf2Uk"
   },
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='ch10', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='ja', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=5, thresholdtgt=5, tokenizer=None, trainpref='train', user_dir=None, validpref='dev', workers=20)\n",
      "| [ja] Dictionary: 60247 types\n",
      "| [ja] train.ja: 440288 sents, 11273745 tokens, 1.43% replaced by <unk>\n",
      "| [ja] Dictionary: 60247 types\n",
      "| [ja] dev.ja: 1166 sents, 25519 tokens, 1.55% replaced by <unk>\n",
      "| [en] Dictionary: 55495 types\n",
      "| [en] train.en: 440288 sents, 12319166 tokens, 1.58% replaced by <unk>\n",
      "| [en] Dictionary: 55495 types\n",
      "| [en] dev.en: 1166 sents, 26091 tokens, 2.85% replaced by <unk>\n",
      "| Wrote preprocessed data to ch10\n"
     ]
    }
   ],
   "source": [
    "!fairseq-preprocess --source-lang ja --target-lang en \\\n",
    "    --trainpref train \\\n",
    "    --validpref dev \\\n",
    "    --destdir ch10  \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq_cli/train.py\", line 333, in cli_main\n",
      "    main(args)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq_cli/train.py\", line 86, in main\n",
      "    train(args, trainer, task, epoch_itr)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq_cli/train.py\", line 127, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/trainer.py\", line 357, in train_step\n",
      "    raise e\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/trainer.py\", line 330, in train_step\n",
      "    sample, self.model, self.criterion, self.optimizer, ignore_grad\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/tasks/fairseq_task.py\", line 251, in train_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 56, in forward\n",
      "    net_output = model(**sample['net_input'])\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/models/fairseq_model.py\", line 223, in forward\n",
      "    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/models/transformer.py\", line 389, in forward\n",
      "    x, encoder_embedding = self.forward_embedding(src_tokens)\n",
      "  File \"/mnt/d/files/git/nlp100_2020/lab/lib/python3.6/site-packages/fairseq/models/transformer.py\", line 356, in forward_embedding\n",
      "    x = embed = self.embed_scale * self.embed_tokens(src_tokens)\n",
      "RuntimeError: \"mul_cpu\" not implemented for 'Half'\n"
     ]
    }
   ],
   "source": [
    "! fairseq-train ch10 \\\n",
    "    --fp16 \\\n",
    "    --save-dir ch10-save \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 91.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Hzf5EQEf48U"
   },
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JjYxtX-f7jM"
   },
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCvxtti8f-CL"
   },
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ORRGeC6gAYb"
   },
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7tQOmZIgELz"
   },
   "source": [
    "## 96. 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbYVowYqgFzL"
   },
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8RiMkQvPgIIz"
   },
   "source": [
    "## 98. ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBHudOURgLgr"
   },
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npZakVeBffuF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ch10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
