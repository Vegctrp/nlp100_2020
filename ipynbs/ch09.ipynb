{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aG4GHjAX_zYq"
   },
   "source": [
    "# 第9章: RNN, CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kd7HS2eIEXYX"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from stemming.porter2 import stem\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RVp2-bdHP5xD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gByKXsH_yJw"
   },
   "source": [
    "## 80. ID番号への変換\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bimMAXQTEOZE"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0zopvj-EvDI"
   },
   "outputs": [],
   "source": [
    "with open(\"../data/NewsAggregatorDataset/newsCorpora.csv\", \"r\") as intxt:\n",
    "    inline = intxt.readlines()\n",
    "    \n",
    "select_publisher = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "datas = []\n",
    "\n",
    "for line in inline:\n",
    "    if line != \"\":\n",
    "        ll = line.split(\"\\t\")\n",
    "        if ll[3] in select_publisher:\n",
    "            datas.append((ll[4],ll[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qh8wKWLRFsaN"
   },
   "outputs": [],
   "source": [
    "def get_feat_from_sentence(string):\n",
    "    sl = nlp.make_doc(string)\n",
    "    string = [i.lemma_.lower() for i in sl]\n",
    "    return string\n",
    "\n",
    "def data2stemmed(data):\n",
    "    return [(label, get_feat_from_sentence(string)) for label,string in data]\n",
    "\n",
    "def get_feature_stems(stemmed):\n",
    "    counter = Counter([tok for _,toks in stemmed for tok in toks])\n",
    "    return [stem for stem,num in counter.most_common() if 2<=num]\n",
    "\n",
    "def make_feature_dic(features):\n",
    "    dic = {}\n",
    "    for i, word in enumerate(features):\n",
    "        dic[word] = i+1\n",
    "    return dic\n",
    "\n",
    "def get_id(stemmed, feature_dic):\n",
    "    if stemmed in feature_dic:\n",
    "        return feature_dic[stemmed]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_ids(stemmedl, feature_dic):\n",
    "    return [get_id(i, feature_dic) for i in stemmedl]\n",
    "\n",
    "def stemmed2ids(stemmed, feature_dic):\n",
    "    return [(label, get_ids(steml, feature_dic)) for label,steml in stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFiFjAneFtHE"
   },
   "outputs": [],
   "source": [
    "stemmed = data2stemmed(datas)\n",
    "features = get_feature_stems(stemmed)\n",
    "feature_dic = make_feature_dic(features)\n",
    "ids = stemmed2ids(stemmed, feature_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Uhxkhnbq9MM",
    "outputId": "929fe171-f6d1-442c-ba1a-923796e7898f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8600"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "apwg79L3JwMZ",
    "outputId": "32f23e15-bed1-48d1-9efb-c198c398a15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(get_id(\"-\", feature_dic))\n",
    "print(get_id(\"hogehoge\", feature_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pcXaXqoSKgR2",
    "outputId": "f65b8147-5973-4310-a9e4-ebb1543afc6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['europe', 'reach', 'crunch', 'point', 'on', 'bank', 'union']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "26nuFOogKoRE",
    "outputId": "83710a36-8d75-43c1-a7c3-000348dfc15e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[247, 858, 0, 927, 13, 60, 957]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ids(stemmed[0][1], feature_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "xsLDIxNaLVlw",
    "outputId": "d23ff200-0f4b-4b87-809e-287a4b8e4ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b', [247, 858, 0, 927, 13, 60, 957])\n",
      "('b', [54, 604, 1, 2586, 43, 0, 61, 54, 5, 1249, 3, 419, 72, 45, 12, 7, 6])\n",
      "('b', [43, 0, 0, 9, 6314, 116, 550, 4, 1900, 243, 4220])\n",
      "('m', [14, 0, 1611, 116, 8600, 4, 669, 1737, 1, 126])\n",
      "('m', [246, 1301, 49, 0, 4943, 4, 126, 65, 8, 4975, 158])\n",
      "('m', [337, 707, 1692, 745, 3, 246, 1, 104, 2123, 2124, 20, 2898, 11, 463, 7, 6])\n"
     ]
    }
   ],
   "source": [
    "for i in ids[:3]:\n",
    "   print(i)\n",
    "\n",
    "for i in ids[-3:]:\n",
    "   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1aGamoNQgEi"
   },
   "outputs": [],
   "source": [
    "def ids2ids_onehot(ids, features):\n",
    "    size = len(features) + 1\n",
    "    identity = np.identity(size)\n",
    "    return [(label, identity[i]) for label,i in tqdm(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SXQi-4VRU1B"
   },
   "outputs": [],
   "source": [
    "#ids_onehot = ids2ids_onehot(ids, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15W-KXrIZ-BT"
   },
   "outputs": [],
   "source": [
    "#for i in ids_onehot[:3]:\n",
    "#   print(i)\n",
    "\n",
    "#for i in ids_onehot[-3:]:\n",
    "#   print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wX40EirAAJw"
   },
   "source": [
    "## 81. RNNによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COu4zhS2S-PR"
   },
   "outputs": [],
   "source": [
    "#random.shuffle(ids_onehot)\n",
    "random.shuffle(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0NW3WD5qILC"
   },
   "outputs": [],
   "source": [
    "class Model81(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, out_dim):\n",
    "        super(Model81, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "        nn.init.normal_(self.embed.weight, 0.0, 1.0)\n",
    "        #self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def predict(self, x, h=None):\n",
    "        x = self.embed(x.to(torch.int64))\n",
    "        x, hp = self.rnn(x, h)\n",
    "        h = self.out(hp)\n",
    "        #print(h)\n",
    "        h = h.squeeze(0)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x):\n",
    "        label = self.predict(x, None)\n",
    "        #return self.loss(label, ans)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OxWAqpVqwRl"
   },
   "outputs": [],
   "source": [
    "model = Model81(8601, 300, 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9jZfbrYyuc2N",
    "outputId": "f7c797c6-cc1e-49df-f770-8fbd219f76b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0797,  0.3024, -0.2443, -0.1584]], grad_fn=<SqueezeBackward1>)\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(torch.tensor(ids[0][1]).unsqueeze(0)))\n",
    "print(ids[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XzOK3maLAN2R",
    "outputId": "0dfda8e2-46cb-449b-e0fa-e18b4ae176d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5165, -0.2733,  0.4425,  0.5172]], grad_fn=<SqueezeBackward1>)\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(torch.tensor(ids[1][1]).unsqueeze(0)))\n",
    "print(ids[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbH_6XPMAFfG"
   },
   "source": [
    "## 82. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gcqDdSTmJuHY",
    "outputId": "4cbc9742-272c-448f-e89c-b38626f48d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data : 10684\n",
      "valid data : 1336\n",
      "test data : 1336\n"
     ]
    }
   ],
   "source": [
    "label_dic = {\"b\":0, \"e\":1, \"m\":2, \"t\":3}\n",
    "\n",
    "def div_vecs(vecs):\n",
    "    x = [np.array(i[1]) for i in vecs]\n",
    "    label = [label_dic[i[0]] for i in vecs]\n",
    "    return x,label\n",
    "\n",
    "trains = ids[:len(ids)*4//5]\n",
    "print(\"train data :\",len(trains))\n",
    "\n",
    "tests = ids[len(ids)*8//10:len(ids)*9//10]\n",
    "print(\"valid data :\",len(tests))\n",
    "\n",
    "valids = ids[len(ids)*9//10:]\n",
    "print(\"test data :\",len(valids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dK0yRMbdKOPg"
   },
   "outputs": [],
   "source": [
    "trainx, trainy = div_vecs(trains)\n",
    "testx, testy = div_vecs(tests)\n",
    "validx, validy = div_vecs(valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lHSunHZKTf2"
   },
   "outputs": [],
   "source": [
    "datasets = [trainx, trainy, testx, testy, validx, validy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EzYx3nXWc-vg"
   },
   "outputs": [],
   "source": [
    "def try_gpu(e):\n",
    "    if torch.cuda.is_available():\n",
    "        return e.cuda()\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGOTSift3nCS"
   },
   "outputs": [],
   "source": [
    "class Model81(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, out_dim):\n",
    "        super(Model81, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "        nn.init.normal_(self.embed.weight, 0.0, 1.0)\n",
    "    \n",
    "    def predict(self, x, h=None):\n",
    "        x = self.forward(x, h)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.embed(x.to(torch.int64))\n",
    "        x, hp = self.rnn(x, h)\n",
    "        h = self.out(hp)\n",
    "        #print(h)\n",
    "        h = h.squeeze(0)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fkrv6Fi85uT9"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, status, use_gpu=False):\n",
    "        dic = {\"train\":0, \"test\":1, \"valid\":2}\n",
    "        self.status = status\n",
    "        vecs = dataset\n",
    "        self.data = vecs[dic[status]*2]\n",
    "        self.label = vecs[dic[status]*2+1]\n",
    "        self.data_num = len(self.data)\n",
    "        self.use_gpu = use_gpu\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label =  self.label[idx]\n",
    "        return out_data, out_label\n",
    "    \n",
    "    def collate(self, batch):\n",
    "        datas, labels = list(zip(*batch))\n",
    "        datas = [torch.tensor(data) for data in datas]\n",
    "        datas = pad_sequence(datas, batch_first=True)\n",
    "        if self.use_gpu:\n",
    "            return try_gpu(datas), try_gpu(torch.tensor(labels))\n",
    "        else:\n",
    "            return datas, torch.tensor(labels)\n",
    "\n",
    "def generator(dataset, status, batch_size, shuffle=True, use_gpu=False):\n",
    "    data_set = Dataset(dataset, status, use_gpu)\n",
    "    return torch.utils.data.DataLoader(data_set, collate_fn = data_set.collate, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tm7yDRhr54vb",
    "outputId": "ab5b1360-e77e-4c4d-cb8f-eb0e524d2a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  10,  801,  345,   12,    2, 1238,    1,  254,   15, 1086,   11,  444,\n",
      "         1009,   28,    9,    0,   11, 3499,   80,    7,    6]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "ds = generator(datasets, \"train\", 1)\n",
    "for x, y in ds:\n",
    "  print(x,y)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YJZDZ6CmMN1h",
    "outputId": "e0b10109-0c7d-4e12-e57b-67fd758af848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 103,   86,   16,  ...,    0,    0,    0],\n",
      "        [  14,   47,    1,  ...,    0,    0,    0],\n",
      "        [  10,  392, 2479,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [3625, 3626,    1,  ...,    0,    0,    0],\n",
      "        [ 186, 4670, 4775,  ...,    0,    0,    0],\n",
      "        [5089,  330,    8,  ...,    0,    0,    0]]) tensor([1, 0, 3, 0, 1, 3, 0, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 3, 0, 3, 1, 0,\n",
      "        3, 3, 0, 1, 1, 1, 0, 1, 2, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 3, 1, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 3, 0, 0, 3, 0, 1, 1, 3, 0, 0, 1, 0, 1, 3, 0, 1, 3, 1, 1, 1, 1,\n",
      "        3, 1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "ds = generator(datasets, \"train\", 100)\n",
    "for x, y in ds:\n",
    "  print(x,y)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj8jiOkYXAMs"
   },
   "outputs": [],
   "source": [
    "def accuracy(dataset, md, use_gpu=False):\n",
    "    with torch.no_grad():\n",
    "        if use_gpu:\n",
    "            predicts = [md.predict(try_gpu(torch.tensor(dat).unsqueeze(0))) for dat in dataset.data]\n",
    "        else:\n",
    "            predicts = [md.predict(torch.tensor(dat).unsqueeze(0)) for dat in dataset.data]\n",
    "        return np.mean([int(p.argmax()==a) for p,a in zip(predicts, dataset.label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKDrDvIy42TB"
   },
   "outputs": [],
   "source": [
    "## batch>=2のときスコアが明らかに悪い(パディングがヤバそう)　要改善\n",
    "class TRAINER:\n",
    "    def __init__(self, model, criterion, vecs, optimizer, gen_batchsize, max_iter, use_gpu=False):\n",
    "        self.train_generator = generator(datasets, \"train\", gen_batchsize, True,  use_gpu)\n",
    "        self.valid_generator = generator(datasets, \"valid\", 1, True, use_gpu)\n",
    "        self.train_dataset = Dataset(datasets, \"train\")\n",
    "        self.valid_dataset = Dataset(datasets, \"valid\")\n",
    "        self.model = model\n",
    "        if use_gpu:\n",
    "            self.model = try_gpu(self.model)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_losses = []\n",
    "        for x,y in self.train_generator:\n",
    "            out = self.model(x)\n",
    "            loss = self.criterion(out, y)\n",
    "            #self.optimizer.zero_grad()\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        return accuracy(self.train_dataset, self.model, self.use_gpu), np.mean(train_losses)\n",
    "        \n",
    "    def valid(self):\n",
    "        self.model.eval()\n",
    "        valid_losses = []\n",
    "        for x, y in self.valid_generator:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(x)\n",
    "                loss = self.criterion(out, y)\n",
    "                valid_losses.append(loss.item())\n",
    "        return accuracy(self.valid_dataset, self.model, self.use_gpu), np.mean(valid_losses)\n",
    "\n",
    "    def learning(self):\n",
    "        for ep in range(self.max_iter):\n",
    "            train_acc, train_loss = self.train()\n",
    "            valid_acc, valid_loss = self.valid()\n",
    "            print(train_acc, valid_acc, train_loss, valid_loss)\n",
    "        return train_acc, valid_acc, train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4aoCr2A66iWS"
   },
   "outputs": [],
   "source": [
    "model = Model81(8601, 300, 50, 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rhkIExaFZad5",
    "outputId": "0b98df97-2200-4e14-e5a8-0958e36896db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2283788843129914"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset(datasets, \"train\")\n",
    "accuracy(train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "a98y_yMAjOLB",
    "outputId": "ab01295f-9419-421e-b950-524c08632ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.7329651815799326 0.6968562874251497 0.9790753210313864 0.8765985816995987\n",
      "0.7616997379258704 0.6923652694610778 0.7898157525766475 0.8894363186980213\n",
      "0.7730250842381131 0.7080838323353293 0.6881962693207351 0.7887042267920734\n",
      "0.801572444777237 0.7335329341317365 0.6126789602504298 0.8201215744227162\n",
      "0.830026207412954 0.7462574850299402 0.5632478011491072 0.769931346654434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.830026207412954, 0.7462574850299402, 0.5632478011491072, 0.769931346654434)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WomFkyUDAIae"
   },
   "source": [
    "## 83. ミニバッチ化・GPU上での学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "HnEWx5v2c6yR",
    "outputId": "4db529c7-3444-4686-dd8a-dd507a1cbb2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Y_mxr1ZjvOe"
   },
   "outputs": [],
   "source": [
    "model = Model81(8601, 300, 50, 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 4, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "-NGNNswgjyAT",
    "outputId": "e90ed1c9-8340-4d46-f28c-1d22d3eef10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.5416510670160989 0.5351796407185628 1.1382277091314443 1.1290728987423246\n",
      "0.5765630849868963 0.5494011976047904 1.1112408380814591 1.089036562262538\n",
      "0.6157806065144141 0.5808383233532934 1.0950248600480619 1.0514136997614791\n",
      "0.6278547360539124 0.5988023952095808 1.049371959564169 1.0268958297496784\n",
      "0.5312616997379259 0.5224550898203593 1.0379044489662372 1.1977723392540822\n",
      "0.662392362411082 0.6392215568862275 1.0663345205743675 1.0203484702788428\n",
      "0.5674840883564208 0.5576347305389222 1.0102407737690515 1.0974903584061981\n",
      "0.5950954698614751 0.5785928143712575 1.1121971204860164 1.0818854991101219\n",
      "0.6106327218270311 0.594311377245509 1.098163985014408 1.0786356266119523\n",
      "0.6334706102583302 0.5995508982035929 1.086937260556337 1.0383929484440182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6334706102583302, 0.5995508982035929, 1.086937260556337, 1.0383929484440182)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model81(8601, 300, 50, 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.7122800449269936 0.6811377245508982 1.0110051939769524 0.9227620379296605\n",
      "0.7519655559715462 0.688622754491018 0.8233362262847495 0.8620346756930837\n",
      "0.799045301385249 0.7297904191616766 0.7301055453871127 0.7879362361516781\n",
      "0.8005428678397604 0.7290419161676647 0.662836114913281 0.7820159638534763\n",
      "0.8200112317484088 0.7425149700598802 0.6067876322846965 0.785502018975819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8200112317484088, 0.7425149700598802, 0.6067876322846965, 0.785502018975819)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LwQ-av8AJtf"
   },
   "source": [
    "## 84. 単語ベクトルの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "okMoqFZid54f",
    "outputId": "49eef2d9-898e-4856-a958-29aaaa02dc87"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "googlenews_w2v = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxE2SUAWhLJQ"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def init_emb_w2v(model):\n",
    "    for i, token in enumerate(features):\n",
    "        if token in googlenews_w2v:\n",
    "            vc = copy.deepcopy(googlenews_w2v[token])\n",
    "            model.embed.weight.data[i] = torch.tensor(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfjTPDT1qZMJ"
   },
   "outputs": [],
   "source": [
    "model = Model81(8601, 300, 50, 4)\n",
    "init_emb_w2v(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "5RNp4TQIq9XG",
    "outputId": "379f6721-0df2-4ecd-8625-f6af38bb433b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.667633845001872 0.625748502994012 1.0264221317438498 1.0009513532988237\n",
      "0.744945713216024 0.6931137724550899 0.8813914419030676 0.8295636649946209\n",
      "0.7675964058405091 0.7238023952095808 0.7443110938247597 0.7984993853013751\n",
      "0.8102770497940847 0.7297904191616766 0.671383581644966 0.7591646424652227\n",
      "0.7967989517034818 0.7312874251497006 0.6248228048183266 0.7885292599853937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7967989517034818,\n",
       " 0.7312874251497006,\n",
       " 0.6248228048183266,\n",
       " 0.7885292599853937)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lME6quBEAMvM"
   },
   "source": [
    "## 85. 双方向RNN・多層化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evoSmV2mtK5t"
   },
   "outputs": [],
   "source": [
    "class Model85(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, out_dim):\n",
    "        super(Model85, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_dim*2, out_dim)\n",
    "        nn.init.normal_(self.embed.weight, 0.0, 1.0)\n",
    "    \n",
    "    def predict(self, x, h=None):\n",
    "        x = self.forward(x, h)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        x = self.embed(x.to(torch.int64))\n",
    "        x, hp = self.rnn(x, h)\n",
    "        h = hp[-2:]\n",
    "        h = h.transpose(0,1)\n",
    "        h = h.contiguous().view(-1, h.size(1) * h.size(2))\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRmIXSU-gSNI"
   },
   "outputs": [],
   "source": [
    "model = Model85(8601, 300, 50, 4)\n",
    "init_emb_w2v(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "QSE7OSFUgWfu",
    "outputId": "ac60487c-7c1f-4d1b-e7b3-4add30227e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.7647884687383003 0.7245508982035929 0.8773009215159225 0.7467689115706062\n",
      "0.8490265818045676 0.7754491017964071 0.606193734710893 0.6142555233059747\n",
      "0.8977910894795956 0.8098802395209581 0.44001128170168397 0.5140026786830494\n",
      "0.9133283414451516 0.8001497005988024 0.3168558874900571 0.5699538128082151\n",
      "0.9269000374391614 0.8143712574850299 0.23804451305247773 0.6041384062741062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9269000374391614,\n",
       " 0.8143712574850299,\n",
       " 0.23804451305247773,\n",
       " 0.6041384062741062)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKzalUjptlFo"
   },
   "outputs": [],
   "source": [
    "model = Model85(8601, 300, 50, 4)\n",
    "init_emb_w2v(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 8, 10, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "slsPPL2fvclK",
    "outputId": "9ead40dc-674d-425b-b483-cc80ff8d6228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.6843878697117185 0.6601796407185628 0.9570134239282436 0.92695612000848\n",
      "0.7208910520404342 0.6841317365269461 0.7932636824932819 0.8341679519521976\n",
      "0.7338075627105953 0.6744011976047904 0.6850977506555483 0.8166256576865732\n",
      "0.8158929239985024 0.7537425149700598 0.5939160730622842 0.6831368532277153\n",
      "0.8390116061400225 0.7649700598802395 0.5103748515805977 0.6672177050195768\n",
      "0.8638150505428679 0.7761976047904192 0.44409274949530464 0.6393207655160013\n",
      "0.8807562710595283 0.7956586826347305 0.3791992886014238 0.6303435368184558\n",
      "0.9128603519281168 0.8031437125748503 0.33079202953882203 0.5759019294006382\n",
      "0.9039685511044553 0.7919161676646707 0.2786156015727156 0.6224816416105824\n",
      "0.9465555971546238 0.812874251497006 0.23896729661162922 0.5855396860641634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9465555971546238,\n",
       " 0.812874251497006,\n",
       " 0.23896729661162922,\n",
       " 0.5855396860641634)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReYIs73wAPKz"
   },
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3i2N5o7JSlWe"
   },
   "outputs": [],
   "source": [
    "class Model86(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, out_dim):\n",
    "        super(Model86, self).__init__()\n",
    "        self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.act = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3)\n",
    "        self.out = nn.Linear(hidden_dim, out_dim)\n",
    "        nn.init.normal_(self.embed.weight, 0.0, 1.0)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x.to(torch.int64))\n",
    "        x = self.conv(x.transpose(-1,-2))\n",
    "        x = self.act(x)\n",
    "        x = F.max_pool1d(x, x.size(-1))\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2k6Gf7jV2vAS"
   },
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vqx-LadLSqJC"
   },
   "outputs": [],
   "source": [
    "model = Model86(8601, 300, 50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oe71QPImStfq",
    "outputId": "6f61542a-1e35-4aff-c5d1-7a9184fbb3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2813, 0.2301, 0.1819, 0.3068]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(torch.tensor(ids[0][1]).unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YyHm3GEkgnBj"
   },
   "outputs": [],
   "source": [
    "model = Model86(8601, 300, 50, 4)\n",
    "init_emb_w2v(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "rrVtBSPPocBE",
    "outputId": "4af9719a-9074-425e-ff26-0eb18b7dc067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n",
      "0.7155559715462374 0.6534431137724551 1.520299936121617 2.336850895940203\n",
      "0.8147697491576189 0.7410179640718563 6.438891968266377 6.224710974875008\n",
      "0.8407899663047548 0.7589820359281437 9.905072320440432 11.071071534945808\n",
      "0.8916136278547361 0.7776946107784432 7.799505887268891 10.71391025770076\n",
      "0.907244477723699 0.7904191616766467 5.352192184770551 14.411713592215547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.907244477723699, 0.7904191616766467, 5.352192184770551, 14.411713592215547)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(param1):\n",
    "    print(\"#################################\")\n",
    "    print(\"lr :\", param1)\n",
    "    model = Model86(8601, 300, 50, 4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=param1)\n",
    "    trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, False)\n",
    "    trainer.learning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################\n",
      "lr : 0.0005\n",
      "0.7641332834144515 0.7260479041916168 0.9085624298658825 0.7925801760953444\n",
      "0.8458442530887308 0.7761976047904192 0.6179975375333033 0.6329990813025367\n",
      "0.9090228378884313 0.8031437125748503 0.43359856729503315 0.5518283525968459\n",
      "0.9584425308873081 0.812874251497006 0.30569945089689055 0.5245845201402435\n",
      "0.9790340696368401 0.8233532934131736 0.21584127549701265 0.5083019131219224\n",
      "#################################\n",
      "lr : 0.005\n",
      "0.8746724073380756 0.7769461077844312 0.7315544986436615 0.6259733908467116\n",
      "0.8446274803444402 0.7223053892215568 0.44217909202098127 1.0277974685956648\n",
      "0.9259640584050918 0.7934131736526946 0.30350672302174136 0.9479423128073134\n",
      "0.9031261699737926 0.75 0.265390673933642 1.560742645719535\n",
      "0.9519842755522276 0.8083832335329342 0.24084692438854516 1.2885715534745141\n",
      "#################################\n",
      "lr : 0.05\n",
      "0.4170722575814302 0.4408682634730539 nan nan\n",
      "0.4170722575814302 0.4408682634730539 nan nan\n",
      "0.4170722575814302 0.4408682634730539 nan nan\n",
      "0.4170722575814302 0.4408682634730539 nan nan\n",
      "0.4170722575814302 0.4408682634730539 nan nan\n"
     ]
    }
   ],
   "source": [
    "for param1 in [5e-4, 5e-3, 5e-2]:\n",
    "    obj(param1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[K     |████████████████████████████████| 769 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from transformers) (4.47.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 926 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 56.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: numpy in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from transformers) (1.19.0)\n",
      "Requirement already satisfied: requests in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: six in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/miyazaki.k/kemvenv/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Using legacy setup.py install for sacremoses, since package 'wheel' is not installed.\n",
      "Installing collected packages: filelock, sentencepiece, click, sacremoses, tokenizers, transformers\n",
      "    Running setup.py install for sacremoses ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed click-7.1.2 filelock-3.0.12 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112223b4237c4fa4a65e74f220d32e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic = {\"b\":0, \"e\":1, \"m\":2, \"t\":3}\n",
    "\n",
    "datas_bert = [(label_dic[data[0]], torch.tensor(tokenizer.encode(data[1]), dtype=torch.long)) for data in datas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data : 10684\n",
      "valid data : 1336\n",
      "test data : 1336\n"
     ]
    }
   ],
   "source": [
    "def div_vecs(vecs):\n",
    "    x = [i[1] for i in vecs]\n",
    "    label = [i[0] for i in vecs]\n",
    "    return x,label\n",
    "\n",
    "trains = datas_bert[:len(datas_bert)*4//5]\n",
    "print(\"train data :\",len(trains))\n",
    "\n",
    "tests = datas_bert[len(datas_bert)*8//10:len(datas_bert)*9//10]\n",
    "print(\"valid data :\",len(tests))\n",
    "\n",
    "valids = datas_bert[len(datas_bert)*9//10:]\n",
    "print(\"test data :\",len(valids))\n",
    "\n",
    "trainx, trainy = div_vecs(trains)\n",
    "testx, testy = div_vecs(tests)\n",
    "validx, validy = div_vecs(valids)\n",
    "\n",
    "datasets = [trainx, trainy, testx, testy, validx, validy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  101,  1980,  5965,   172, 22715,  1553,  1113,  9339,  3779,   102]),\n",
       " tensor([  101, 16028,  2064,   143,  9244, 13329,   118, 11661,  1200, 27772,\n",
       "         22592,  1116,  1149, 16028,  2064,   112,   188,  3802,  1106,  1712,\n",
       "          5600,  1822,  1111,   119,   119,   119,   102]),\n",
       " tensor([  101, 11854,  1760,  8745, 26883,  1279, 17878,  1162,  1112,   139,\n",
       "         20408,  3299,   157, 11811,  6385,  3377,   117,  2722,  3177, 21238,\n",
       "         16890, 14367,  1116,   102]),\n",
       " tensor([  101,  1302,  7904,  8652,  1116, 11661, 11854,   140, 15998,  1116,\n",
       "         12118,  7200,  6922,  1174,  6051,  2544,  3313,   113,   122,   114,\n",
       "           102]),\n",
       " tensor([  101,   155, 14663, 17656,  2036,   118,  6304,  4891,  9887,  1116,\n",
       "          2501,  2672,  1107, 16028,  2064,  3085,  2774,  8679,   118,  3509,\n",
       "           102])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = transformers.BertConfig.from_pretrained('bert-base-cased', num_labels=4)\n",
    "        self.bert = transformers.BertForSequenceClassification.from_pretrained('bert-base-cased', config=config)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.bert(batch)\n",
    "        return x[0]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy / valid_accuracy / train_loss / valid_loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/miyazaki.k/kemvenv/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8143953575439911 0.812874251497006 0.5683475751492558 0.502018268950685\n",
      "0.8893672781729689 0.8525449101796407 0.41556599898000907 0.4220233768849316\n",
      "0.9262448521153126 0.8997005988023952 0.2831514143053399 0.3331132270172685\n",
      "0.9427180831149382 0.9019461077844312 0.22055188061279646 0.29990833734502337\n",
      "0.95376263571696 0.8952095808383234 0.18142005102168482 0.3204194675841017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.95376263571696, 0.8952095808383234, 0.18142005102168482, 0.3204194675841017)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "trainer = TRAINER(model, criterion, datasets, optimizer, 1, 5, True)\n",
    "\n",
    "print(\"train_accuracy / valid_accuracy / train_loss / valid_loss\")\n",
    "trainer.learning()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ch09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
